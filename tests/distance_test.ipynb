{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TEST-SCRIPT for string distance measures and alignments\n",
    "\n",
    "This package implements sequence -2- sequence aligning and matching for character strings.\n",
    "It is an implementation of general DP with SUB/INS/DEL moves and with local distance being the equality test (x_i == y_j). \n",
    "The matching can be done at the character or word level by invoking different tokenization options.\n",
    "   \n",
    "Applications include aligning strings, measuring error rates in ASR, finding typos, ...\n",
    "\n",
    "All algorithms have a computational cost O(N^2) as they are variants on the Dynamic Programming principle.\n",
    "Memory cost is O(N^3) for those routines allowing for backtracking and aligning.  Memore costs is O(N^2) for the routines\n",
    "that strictly compute a distance.\n",
    "\n",
    "The term 'measure' is intentional as these measures are not 'metrics' in a mathematical sense.\n",
    "\n",
    "- levenshtein():\n",
    "   + computes the Levenshtein distance, i.e. #S+#I+#D\n",
    "   + this is a single (forward) pass algorithm that only yields the composite distance, no alignment, no backtracking \n",
    "      \n",
    "- edit_distance(): \n",
    "    - computes the weighted edit distance\n",
    "    - allowing Substitutions, Insertion, Deletions and Compounds. \n",
    "    - the detailed edit counts\n",
    "    - optionally an alignment is computed via backtracking (forward + backward pass) \n",
    "    \n",
    "- align():\n",
    "    - returns the sequence alignment\n",
    "\n",
    "- levenshtein() and edit_distance() \n",
    "    - accept a single pair of strings or a set of matching pairs \n",
    "    - they return\n",
    "        - edit distance (composite over the full set)\n",
    "        - a results structure with details\n",
    "        \n",
    "- levenshtein_t() and edit_distance_t()\n",
    "    - are underlying routines that take a single pair of tokens as input\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## levenshtein() and edit_distance() details \n",
    "- TOKENIZATION: text strings are first converted to a list of tokens before using these modules.   Tokenization options are:\n",
    "    - TOKEN = None (default): string is converted to list of characters\n",
    "    - TOKEN = \"WORD\"\n",
    "    - TOKEN = \"CHAR\"\n",
    "- Compounding: The edit_distance() routine allows for compounding if **CMPND** characters are defined.  By default there is no compounding and CMPND=[].  Compounding is naive by allowing 2 word compounds only and by defining compounding rules by compounding characters, including '' for cross white space compounding.  The most common option is to allow for simple and dash compounding by defining CMPND=\\['','-'\\]\n",
    "\n",
    "- nomenclature\n",
    "  + number_of_edits: the number of edits (sum of substitutions, insertions and deletions) making abstraction of compounds\n",
    "  + weighted_edit_distance: weighted sum of edits (possibly including compounds)\n",
    "  + hypothesis, reference:   edits are applied on the reference to yield the hypothesis\n",
    "  + alignment: an alignment made between reference and hypothesis'\n",
    "  + trellis: 2D arrangement of alignment/alignment scores used in the DP\n",
    "  \n",
    "- flags and options:\n",
    "  + VERBOSE:  to print intermediate results\n",
    "  + ALIGN:       to do backtracking and return an alignment on top of the scores\n",
    "  + TOKEN:    tokenization option\n",
    "    \n",
    "- results:   \n",
    "Both routines return a dictionary of results. The elements depend on the selected routine and options and may include:\n",
    ">    + 'total': number of edits\n",
    ">    + 'ins': number of insertions\n",
    ">    + 'sub': number of substitutions\n",
    ">    + 'del': number of deletions\n",
    ">    + 'comp': number of compounds\n",
    ">    + 'err': error rate (in %)\n",
    ">    + 'hyp_tokens': tokens in hypothesis\n",
    ">    + 'ref_tokens': tokens in reference\n",
    ">    + 'utterances': number of processed utterances\n",
    ">    + 'errors': list of errors\n",
    ">    + 'align' : list of dataframes with utterance alignments\n",
    ">    + 'edits' : list of edits ('H','S','I','D','C')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do all the imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import sys, os\n",
    "\n",
    "sys.path.insert(0,os.path.abspath('..'))\n",
    "import evalign as eva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment of strings with **align()**\n",
    "Aligning sequence is one of the primary goals of the available sequence-2-sequence matching routines.   \n",
    "**align()** is a convenience routine that simply returns such alignment. Under the hood it is a wrapper around edit_distance() .   \n",
    "It is possible to pass extra \\*\\*kwargs arguments to the underlying routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('*', 'w'),\n",
       " ('r', 'r'),\n",
       " ('e', 'e'),\n",
       " ('c', 'c'),\n",
       " ('*', 'k'),\n",
       " ('*', ' '),\n",
       " ('o', 'a'),\n",
       " ('g', ' '),\n",
       " ('n', 'n'),\n",
       " ('i', 'i'),\n",
       " ('z', 'c'),\n",
       " ('e', 'e'),\n",
       " (' ', ' '),\n",
       " ('s', '*'),\n",
       " ('p', 'b'),\n",
       " ('e', 'e'),\n",
       " ('e', 'a'),\n",
       " ('c', 'c'),\n",
       " ('h', 'h')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt1 = \"recognize speech\"\n",
    "utt2 = \"wreck a nice beach\"\n",
    "eva.align(utt1,utt2,EPS=\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('recognize', 'wreck'), ('a', 'a'), ('*', 'nice'), ('speech', 'beach')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt1 = \"recognize a speech\"\n",
    "utt2 = \"wreck a nice beach\"\n",
    "eva.align(utt1,utt2,TOKEN=\"WORD\",EPS=\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein Distance\n",
    "\n",
    "The Levenshtein() distance computes the number of elementary edits (SUB, INS, DEL) to convert one sequence into another.  It is the simplest of all sequence distance measures.  Moreover it is a symmetric distance measure, hence x and y can be reversed.   \n",
    "The levenshtein() function takes strings or list of strings as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function levenshtein in module evalign.distance:\n",
      "\n",
      "levenshtein(x, y, TOKEN=None, **kwargs)\n",
      "     Finds the symmetric Levenshtein distance (sum of SUB/INS/DEL) between two strings of across a corpus \n",
      "     of x-y pairs.    x and y position are irrelevant.\n",
      "     There is no backtracking, and no separate tracking of SUB/INS/DEL \n",
      "     \n",
      "     Note:\n",
      "     + for token counts 'x' is designed as 'hyp and 'y' as 'ref'\n",
      "     + for error rate computation: average length of x and y is used in the division\n",
      "     \n",
      "     Parameters\n",
      "     ----------\n",
      "         x : str or list of str\n",
      "    \n",
      "         y : str or list of str\n",
      "    \n",
      "         TOKEN: None or str (\"WORD\" or \"CHAR\")\n",
      "             see utils.tokenizer()\n",
      "         \n",
      "         **kwargs:\n",
      "             passed to levenshtein()\n",
      "     \n",
      "     Returns\n",
      "    --------\n",
      "         lev_dist:     int\n",
      "             Levenshtein Distance (combined)\n",
      "             \n",
      "         results:      a dictionary with        \n",
      "                         'hyp_tokens' : (int) number of tokens in hypothesis (x)\n",
      "                         'ref_tokens' : (int) number of tokens in reference  (y)\n",
      "                         'total'      : (int) number of edits \n",
      "                         'err'        : (float) error rate (in %)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(eva.levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levensthein Distance (\"bleker\",\"broek\"): 4 \n"
     ]
    }
   ],
   "source": [
    "x = \"bleker\"\n",
    "y = \"broek\" \n",
    "lev_dist,_ = eva.levenshtein(x,y)\n",
    "print(\"Levensthein Distance (\\\"%s\\\",\\\"%s\\\"): %d \" %(x,y,lev_dist) )\n",
    "assert(lev_dist==4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein and other DP routines have roughly a O(N^2) behavior\n",
      "Execution time for moderate sized strings ----- \n",
      "Wall time: 7.11 ms\n",
      "Execution time for strings 10 times the original size\n",
      "Wall time: 546 ms\n",
      "Observe execution time is roughly *100, i.e. O(N^2) behavior\n"
     ]
    }
   ],
   "source": [
    "# O(N^2) time behavior of Levenshtein and other DP routines\n",
    "print(\"Levenshtein and other DP routines have roughly a O(N^2) behavior\")\n",
    "print(\"Execution time for moderate sized strings ----- \")\n",
    "%time ld1= eva.levenshtein(x*10,y*10)\n",
    "print(\"Execution time for strings 10 times the original size\")\n",
    "%time ld10 = eva.levenshtein(x*100,y*100)\n",
    "print(\"Observe execution time is roughly *100, i.e. O(N^2) behavior\")\n",
    "#ld1, ld10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. CHARACTER and WORD TOKENS\n",
    "\n",
    "Sequence-2-sequence matching is used both for measuring a distance between 2 sequences and for aligning them.   \n",
    "A Levenshtein distance measures with how many edits when sequence is converted into the other.\n",
    "The distance itself may be computed with a forward pass only (as is the case in this implementation of Levenshtein). \n",
    "An alignment is obtained after backtracking.  \n",
    "\n",
    "The sequence matching routines levenshtein(), align() and edit_distance() all take both **list of strings** and **strings** as inputs that pass tokenized versions of the inputs to underlying levenshtein_t() and edit_distance_t() routines.\n",
    "\n",
    "Tokenization is done by the utils.tokenizer() routine, where tokenization is done based on the TOKEN argument.\n",
    "- TOKEN is None : string -> list conversion\n",
    "- TOKEN==\"WORD\": word tokenization (white space disappears)\n",
    "- TOKEN==\"CHAR\": character tokenization (after removal of extraneous white space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(eva.utils.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Levensthein Distance (\"Yesterday I read a nice book\",\"Jeremy got a nice book again\"): 18 \n",
      "[('Y', '_'), ('e', '_'), ('s', '_'), ('t', 'J'), ('e', 'e'), ('r', 'r'), ('d', 'e'), ('a', 'm'), ('y', 'y'), (' ', '_'), ('I', '_'), (' ', ' '), ('r', '_'), ('e', 'g'), ('a', 'o'), ('d', 't'), (' ', ' '), ('a', 'a'), (' ', ' '), ('n', 'n'), ('i', 'i'), ('c', 'c'), ('e', 'e'), (' ', ' '), ('b', 'b'), ('o', 'o'), ('o', 'o'), ('k', 'k'), ('_', ' '), ('_', 'a'), ('_', 'g'), ('_', 'a'), ('_', 'i'), ('_', 'n')]\n",
      "Word Levensthein Distance (\"Yesterday I read a nice book\",\"Jeremy got a nice book again\"): 4 \n",
      "[('Yesterday', '_'), ('I', 'Jeremy'), ('read', 'got'), ('a', 'a'), ('nice', 'nice'), ('book', 'book'), ('_', 'again')]\n"
     ]
    }
   ],
   "source": [
    "x = \"Yesterday I read a nice book\"\n",
    "y =  \"Jeremy got a nice book again\"\n",
    "#\n",
    "lev_dist, _ = eva.levenshtein(x,y)\n",
    "align_char = eva.align(x,y)\n",
    "print(\"Character Levensthein Distance (\\\"%s\\\",\\\"%s\\\"): %d \" %(x,y,lev_dist) )\n",
    "print(align_char)\n",
    "#\n",
    "lev_dist, _ = eva.levenshtein(x,y,TOKEN=\"WORD\")\n",
    "align_word = eva.align(x,y,TOKEN=\"WORD\")\n",
    "print(\"Word Levensthein Distance (\\\"%s\\\",\\\"%s\\\"): %d \" %(x,y,lev_dist) )\n",
    "print(align_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW Character Levensthein Distance (\"broek. \",\"   bleker\"): 7 \n",
      "with alignment: \n",
      "[('*', ' '), ('*', ' '), ('*', ' '), ('b', 'b'), ('r', '*'), ('o', 'l'), ('e', 'e'), ('k', 'k'), ('.', 'e'), (' ', 'r')]\n",
      "Character Levensthein Distance (\"broek. \",\"   bleker\"): 7 \n",
      "with alignment: \n",
      "[('*', ' '), ('*', ' '), ('*', ' '), ('b', 'b'), ('r', '*'), ('o', 'l'), ('e', 'e'), ('k', 'k'), ('*', 'e'), (' ', 'r')]\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZATION: string -> list\n",
    "y = \"   bleker\"\n",
    "x = \"broek. \" \n",
    "lev_dist, _ = eva.levenshtein(x,y)\n",
    "print(\"RAW Character Levensthein Distance (\\\"%s\\\",\\\"%s\\\"): %d \" %(x,y,lev_dist) )\n",
    "align_char = eva.align(x,y,EPS='*')\n",
    "print(\"with alignment: \")\n",
    "print(align_char)\n",
    "# TOKENIZATION: string -> meaningfull characters\n",
    "lev_dist, _ = eva.levenshtein(x,y,TOKEN=\"CHAR\")\n",
    "print(\"Character Levensthein Distance (\\\"%s\\\",\\\"%s\\\"): %d \" %(x,y,lev_dist) )\n",
    "align_char = eva.align(x,y,TOKEN=\"CHAR\",EPS='*')\n",
    "print(\"with alignment: \")\n",
    "print(align_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Edit Distance\n",
    "**edit_distance()** is a more versatile sequence distance metric than Levenshtein.  \n",
    "It minimizes the weighted sum of edits: **wS x #SUB + wI x #INS + wD x #DEL**.   \n",
    "Within the *evalign* package edit_distance() is the sequence matching routine of choice to perform  alignment, error rate computations, ...\n",
    "\n",
    "In practice more often than not the total number of edits in edit_distance is identical to Levenshtein.\n",
    "However, levenshtein() often leads to ambiguous alignments and specific edit counts which are dependent on arbitrary code variations.\n",
    "By using slightly non-uniform weights in edit_distance() almost all ambiguities are resolved in a natural way.\n",
    "\n",
    "**edit_distance(x,y)** computes the distance between **x as hypothesis** and **y as reference**.  \n",
    "- One needs to distinguish 'reference' and 'hypothesis' explicitly as the weighted edit_distance is not symmetric by definition (though in practice it often is) \n",
    "- EDITS are defined with respect to the REFERENCE\n",
    "- \n",
    "\n",
    "This module returns a results dictionary,including:\n",
    "- *total*:  the number of edits required to match both sequences subject to the weighted edit criterion\n",
    "- *sub*,*ins*,*del*: the #SUB, #INS, #DEL\n",
    "- *edit_dist*: the weighted edit distance measure\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Matching using Weighted Edit Distance\n",
      "['w', 'o', 'r', 'k'] vs. ['s', 'w', 'o', 'r', 'd']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.4,\n",
       " {'total': 4,\n",
       "  'sub': 4,\n",
       "  'ins': 0,\n",
       "  'del': 0,\n",
       "  'edit_dist': 4.4,\n",
       "  'err': 100.0,\n",
       "  'hyp_tokens': 4,\n",
       "  'ref_tokens': 4,\n",
       "  'utterances': 4})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = \"work\" # \"recognizer\"    # \"to recognize speech\"\n",
    "ref = \"sword\" #\"wreck a nice\"  # \"to wreck a nice beach\"\n",
    "#\n",
    "print(\"String Matching using Weighted Edit Distance\")\n",
    "x = eva.tokenizer(tst,TOKEN=\"CHAR\")\n",
    "y = eva.tokenizer(ref,TOKEN=\"CHAR\")\n",
    "print(x,'vs.',y)\n",
    "eva.edit_distance(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Matching using Weighted Edit Distance\n",
      "work vs. sword\n",
      "\n",
      "Weights : [1.0, 1.0, 1.0]\n",
      "Weighted Edit Distance: 2.00 \n",
      "Error Rate: 40.00% \n",
      "Error Details: #S=1 #I=0 #D=1\n",
      "Edit Distance: 2.00 \n",
      "Tokens (HYP): 4    (REF): 5 \n",
      "Utterances: 1\n",
      "\n",
      "Weights : [2.5, 1.0, 1.0]\n",
      "Weighted Edit Distance: 3.00 \n",
      "Error Rate: 60.00% \n",
      "Error Details: #S=0 #I=1 #D=2\n",
      "Edit Distance: 3.00 \n",
      "Tokens (HYP): 4    (REF): 5 \n",
      "Utterances: 1\n"
     ]
    }
   ],
   "source": [
    "# edits and edit-distance can change when modifying the weights\n",
    "x = \"work\" # \"recognizer\"    # \"to recognize speech\"\n",
    "y = \"sword\" #\"wreck a nice\"  #  \"to wreck a nice beach\"\n",
    "#\n",
    "print(\"String Matching using Weighted Edit Distance\")\n",
    "print(x,'vs.',y)\n",
    "\n",
    "wI = 1.\n",
    "wD = 1.\n",
    "for wS in [1., 2.5 ]:\n",
    "    _,results = eva.edit_distance(x,y,wI=wI,wD=wD,wS=wS)\n",
    "    #\n",
    "    print(\"\\nWeights :\",[wS, wI, wD])\n",
    "    print(\"Weighted Edit Distance: %.2f \" % results['edit_dist'])\n",
    "    eva.pp_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignments and Edits from **edit_distance()**\n",
    "With the **ALIGN** flag turned on an **alignment of (x,y)** as well as a **list of edits** are added to the results dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Level Matchng and Alignment\n",
      "Error Rate: 42.86% \n",
      "Error Details: #S=5 #I=1 #D=3\n",
      "Edit Distance: 9.50 \n",
      "Tokens (HYP): 19    (REF): 21 \n",
      "Utterances: 1\n",
      "[('t', 't'), ('o', 'o'), (' ', ' '), ('_', 'w'), ('r', 'r'), ('e', 'e'), ('c', 'c'), ('_', 'k'), ('_', ' '), ('o', 'a'), ('g', ' '), ('n', 'n'), ('i', 'i'), ('z', 'c'), ('e', 'e'), (' ', ' '), ('s', '_'), ('p', 'b'), ('e', 'e'), ('e', 'a'), ('c', 'c'), ('h', 'h')]\n",
      "['H', 'H', 'H', 'D', 'H', 'H', 'H', 'D', 'D', 'S', 'S', 'H', 'H', 'S', 'H', 'H', 'I', 'S', 'H', 'S', 'H', 'H']\n",
      "\n",
      "Word Level Matching and Alignment\n",
      "Error Rate: 80.00% \n",
      "Error Details: #S=2 #I=0 #D=2\n",
      "Edit Distance: 4.20 \n",
      "Tokens (HYP): 3    (REF): 5 \n",
      "Utterances: 1\n",
      "[('to', 'to'), ('_', 'wreck'), ('_', 'a'), ('recognize', 'nice'), ('speech', 'beach')]\n",
      "['H', 'D', 'D', 'S', 'S']\n"
     ]
    }
   ],
   "source": [
    "x = \"to recognize speech\"\n",
    "y =  \"to wreck a nice beach\"\n",
    "print(\"Character Level Matchng and Alignment\")\n",
    "#\n",
    "_,results = eva.edit_distance(x,y,ALIGN=True)\n",
    "eva.pp_results(results)\n",
    "print(results['align'])\n",
    "print(results['edits'])\n",
    "#\n",
    "print(\"\\nWord Level Matching and Alignment\")\n",
    "_,results = eva.edit_distance(x,y,TOKEN=\"WORD\",ALIGN=True)\n",
    "eva.pp_results(results)\n",
    "print(results['align'])\n",
    "print(results['edits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **edit_distance()** with Compounding\n",
    "The edit_distance() module has an option for accepting naive compounding.  In practice a small edit weight (default=0.2) is assigned to a compound match and compound matches are not counted in the total number of edits nor error rate\n",
    "\n",
    "Naive compounding in this context is defined as simple 2-word compounding with all acceptable compounding characters are specified in a CMPND list ; e.g. CMPND = \\['','-'\\] (glueing allowed over white space or dash).  Compounding is allowed equally in test and reference.\n",
    "\n",
    "It is noted that if more sophisticated compounding is desired, that this should be tackled by preprocessing instead of the matching routine.\n",
    "\n",
    "An extra complicating factor for computing error rates, when allowing for compounding is that the sequence lengths of test and reference may change due to compounding.  As in practice this is more frequent on the test (eg ASR output) and (very) rare on the reference we consider this a minor issue.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Edit Distance allowing for Compounds\n",
      "Error Rate: 40.00% \n",
      "Error Details: #S=0 #I=0 #D=2\n",
      "Accepted Compounds: #C=2\n",
      "Edit Distance: 2.40 \n",
      "Tokens (HYP): 5    (REF): 5 \n",
      "Utterances: 1\n",
      "[('black+boards', 'blackboards'), ('and', 'and'), ('filter+banks', 'filterbanks'), ('_', 'are'), ('_', '...')]\n"
     ]
    }
   ],
   "source": [
    "y = \"blackboards and filterbanks are ...\"\n",
    "x =  \"black boards and filter banks\"\n",
    "#\n",
    "print(\"Weighted Edit Distance allowing for Compounds\")\n",
    "_,results = eva.edit_distance(x,y,TOKEN=\"WORD\",ALIGN=True,CMPND=[''])\n",
    "#\n",
    "eva.pp_results(results)\n",
    "print(results['align'])\n",
    "assert(results['total']==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Edit Distance with Regular and Dash Compounding\n",
      "Error Rate: 20.00% \n",
      "Error Details: #S=1 #I=1 #D=0\n",
      "Accepted Compounds: #C=2\n",
      "Edit Distance: 2.50 \n",
      "Tokens (HYP): 11    (REF): 10 \n",
      "Utterances: 1\n",
      "[('ik', 'ik'), ('hou', 'hou'), ('ervan', 'er+van'), ('naar', 'naar'), ('luister', 'luister-'), ('en', 'en'), ('kamer+muziek', 'kamer-muziek'), ('te', 'te'), ('blijven', '_'), ('luisteren', 'luisteren')]\n",
      "['H', 'H', 'C', 'H', 'S', 'H', 'C', 'H', 'I', 'H']\n"
     ]
    }
   ],
   "source": [
    "y = \"ik hou er van naar luister- en kamer-muziek te luisteren\"\n",
    "x =  \"ik hou ervan naar luister en kamer muziek te blijven luisteren\"\n",
    "#\n",
    "print(\"Weighted Edit Distance with Regular and Dash Compounding\")\n",
    "_,results = \\\n",
    "    eva.edit_distance(x,y,TOKEN=\"WORD\",ALIGN=True,CMPND=['','-'])\n",
    "#\n",
    "eva.pp_results(results)\n",
    "print(results['align'])\n",
    "print(results['edits'])\n",
    "#eva.print_align(results['align'][0])\n",
    "assert(results['total']==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Corpus Processing\n",
    "The core routines process single strings or single lists of tokens.\n",
    "Conglomerate versions , working with multiple utterances or lists, are available in the modules with extension \"\\_x\"\n",
    "These expanded versions always yield a result structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levensthein Distance on a per sentence basis\n",
      "(broek  vs    beekje):  7 \n",
      "(brokken maken vs zandbak):  10 \n",
      "( vs ):  0 \n",
      "Levenshtein Distance (corpus) :  17 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\compi\\Nextcloud\\github\\evalign\\evalign\\distance.py:66: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  err = 200.*total/(hyp_tokens+ref_tokens)\n"
     ]
    }
   ],
   "source": [
    "y = [\"   beekje\",\"zandbak\",\"\"]\n",
    "x = [\"broek \" , \"brokken maken\",\"\"]\n",
    "print(\"Levensthein Distance on a per sentence basis\")\n",
    "for xx,yy in zip(x,y):\n",
    "    lev_dist,_ = eva.levenshtein(xx,yy)\n",
    "    print(\"(%s vs %s):  %d \" %(xx,yy,lev_dist))    \n",
    "\n",
    "_,lev_results = eva.levenshtein(x,y)\n",
    "print(\"Levenshtein Distance (corpus) :  %d \" %lev_results['total'])\n",
    "assert(lev_results['total']==17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Matching using Weighted Edit Distance\n",
      "Processing sentence by sentence\n",
      "work sword 1.1 1.0 1.0\n",
      "{'total': 2, 'sub': 1, 'ins': 0, 'del': 1, 'edit_dist': 2.1, 'err': 40.0, 'hyp_tokens': 4, 'ref_tokens': 5, 'utterances': 1}\n",
      "eight weights 1.1 1.0 1.0\n",
      "{'total': 2, 'sub': 0, 'ins': 0, 'del': 2, 'edit_dist': 2.0, 'err': 28.571428571428573, 'hyp_tokens': 5, 'ref_tokens': 7, 'utterances': 1}\n",
      "recognize speech wreck a nice beach 1.1 1.0 1.0\n",
      "{'total': 9, 'sub': 5, 'ins': 1, 'del': 3, 'edit_dist': 9.499999999999998, 'err': 50.0, 'hyp_tokens': 16, 'ref_tokens': 18, 'utterances': 1}\n",
      "\n",
      "======================= \n",
      "Corpus Processing\n",
      "Error Rate: 43.33% \n",
      "Error Details: #S=6 #I=1 #D=6\n",
      "Edit Distance: 13.60 \n",
      "Tokens (HYP): 25    (REF): 30 \n",
      "Utterances: 3\n"
     ]
    }
   ],
   "source": [
    "x = [\"work\", \"eight\", \"recognize speech\" ]   \n",
    "y = [\"sword\", \"weights\", \"wreck a nice beach\"]\n",
    "#\n",
    "print(\"String Matching using Weighted Edit Distance\")\n",
    "wS=1.1\n",
    "wI=1.0\n",
    "wD=1.0\n",
    "print(\"Processing sentence by sentence\")\n",
    "for xx,yy in zip(x,y):\n",
    "    print(xx,yy,wS,wI,wD)\n",
    "    _,results = eva.edit_distance(xx,yy,wI=wI,wD=wD,wS=wS)\n",
    "    print(results)\n",
    "    \n",
    "print(\"\\n======================= \")\n",
    "print(\"Corpus Processing\")\n",
    "_,results = eva.edit_distance(x,y,ALIGN=True,wI=wI,wD=wD,wS=wS)\n",
    "eva.pp_results(results)\n",
    "assert(results['total']==13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Edit Distance with Compounding for Corpus\n",
      "\n",
      "SUMMARY OF RESULTS\n",
      "==================\n",
      "Error Rate: 21.43% \n",
      "Error Details: #S=1 #I=1 #D=1\n",
      "Accepted Compounds: #C=4\n",
      "Edit Distance: 5.10 \n",
      "Tokens (HYP): 16    (REF): 14 \n",
      "Utterances: 2\n",
      "\n",
      "ALIGNMENTS\n",
      "===============\n",
      "[('ik', 'ik'), ('hou', 'hou'), ('ervan', 'er+van'), ('naar', 'naar'), ('luister', 'luister-'), ('en', 'en'), ('kamer+muziek', 'kamer-muziek'), ('te', 'te'), ('blijven', '_'), ('luisteren', 'luisteren')]\n",
      "[('black+boards', 'blackboards'), ('and', 'and'), ('filter+banks', 'filterbanks'), ('_', 'are')]\n"
     ]
    }
   ],
   "source": [
    "y = [\"ik hou er van naar luister- en kamer-muziek te luisteren\",\"blackboards and filterbanks are \"]\n",
    "x = [\"ik hou ervan naar luister en kamer muziek te blijven luisteren\",\"black boards and filter banks\"]\n",
    "#\n",
    "print(\"Weighted Edit Distance with Compounding for Corpus\")\n",
    "#x = [eva.tokenizer(utt,TOKEN=\"WORD\") for utt in tst]\n",
    "#y = [eva.tokenizer(utt,TOKEN=\"WORD\") for utt in ref]\n",
    "_,results = \\\n",
    "    eva.edit_distance(x,y,ALIGN=True,TOKEN=\"WORD\",CMPND=['','-'],VERBOSE=False,wI=2,wD=1,wS=1.3)\n",
    "#\n",
    "print(\"\\nSUMMARY OF RESULTS\\n==================\")\n",
    "eva.pp_results(results)\n",
    "print(\"\\nALIGNMENTS\\n===============\")\n",
    "for al in results['align']: print(al) # eva.print_align(al)\n",
    "assert(round(results['err'],2)==21.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('ik', 'ik'),\n",
       "  ('hou', 'hou'),\n",
       "  ('ervan', 'er+van'),\n",
       "  ('naar', 'naar'),\n",
       "  ('luister', 'luister-'),\n",
       "  ('en', 'en'),\n",
       "  ('kamer+muziek', 'kamer-muziek'),\n",
       "  ('te', 'te'),\n",
       "  ('blijven', '<>'),\n",
       "  ('luisteren', 'luisteren')],\n",
       " [('black+boards', 'blackboards'),\n",
       "  ('and', 'and'),\n",
       "  ('filter+banks', 'filterbanks'),\n",
       "  ('<>', 'are')]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al = eva.align(x,y,TOKEN=\"WORD\",CMPND=['','-'],EPS='<>') # \\u03bs = small eps char; \\u2205 = empty symbol set char')\n",
    "al"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
